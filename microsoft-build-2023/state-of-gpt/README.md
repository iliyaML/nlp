References:

- [20230523 State of GPT by Andrej Karpathy](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)
- [Notes](https://iliya.web.app/notes/nlp/state-of-gpt-2023/)

---

1. Training
   - GPT Assistant Training Pipeline
     1. Pre-training
        - Data Collection
        - Tokenization
        - 2 Example Models
        - Pre-training
          - Training Process
          - Training Curve
        - Base Models Learn Powerful, General Representations (GPT-1)
        - Base Models can be Prompted into Completing Tasks (GPT-2)
        - Base Models in the Wild
        - Base Models are NOT ‘Assistants’
     2. Supervised Fine-tuning (SFT)
        - SFT Dataset
     3. Reward Modeling
        - RM Dataset
        - RM Training
     4. Reinforcement Learning from Human Feedback (RLHF)
        - RL Training
        - Why RLHF?
        - Mode Collapse
        - Assistant Models in the Wild
2. Applications
   - Human Text Generation vs. LLM Text Generation
   - Chain of Thought
   - Ensemble Multiple Attempts
   - Ask for Reflection
   - Recreate Our ‘System 2’
   - Chains / Agents
   - Condition on Good Performance
   - Tool Use / Plugins
   - Retrieval-Augmented LLMs
   - Constrained Prompting
   - Fine-tuning
   - Default Recommendations
   - Use Cases
   - GPT-4
   - OpenAI API
